{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of object_detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gpandu/Object-detection/blob/master/custom_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q_MQUJCX4xP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y -qq protobuf-compiler python-pil python-lxml\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib PyDrive\n",
        "\n",
        "!pip install -q pycocotools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w9jwHjLYHU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone --quiet https://github.com/tensorflow/models.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRgYyCMuSKRJ",
        "colab_type": "code",
        "outputId": "eafbf8df-9394-406c-974d-2a8039133fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/models"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_P8qMS2IziUS",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/models/research')\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/models/research/slim/')\n",
        "sys.path.append('/content/models/research/object_detection/')\n",
        "sys.path.append('/content/models/research/')\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/:/content/models/research/object_detection/'\n",
        "\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "!python object_detection/builders/model_builder_test.py\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kHGfdQl_76U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ~/datalab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvPtiOyr2JJ-",
        "colab_type": "code",
        "outputId": "cb687828-4180-466b-e830-b4461d993513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "%cd ~/datalab\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "MODEL = 'ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03'\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = 'pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "  opener = urllib.request.URLopener()\n",
        "  opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "  shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/datalab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgdEGfXv_9f0",
        "colab_type": "code",
        "outputId": "655e86d1-a1ab-4873-baf4-45f821d0d911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd ~/datalab/pretrained_model/saved_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/datalab/pretrained_model/saved_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYu7ASNibgat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Binary to run train and evaluation on object detection model.\"\"\"\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import flags\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection import model_hparams\n",
        "from object_detection import model_lib\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    'model_dir', '/root/datalab/trained', 'Path to output model directory '\n",
        "    'where event and checkpoint files will be written.')\n",
        "flags.DEFINE_string('pipeline_config_path', 'ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config', 'Path to pipeline config '\n",
        "                    'file.')\n",
        "flags.DEFINE_integer('num_train_steps', 3000, 'Number of train steps.')\n",
        "flags.DEFINE_boolean('eval_training_data', False,\n",
        "                     'If training data should be evaluated for this job. Note '\n",
        "                     'that one call only use this in eval-only mode, and '\n",
        "                     '`checkpoint_dir` must be supplied.')\n",
        "flags.DEFINE_integer('sample_1_of_n_eval_examples', 1, 'Will sample one of '\n",
        "                     'every n eval input examples, where n is provided.')\n",
        "flags.DEFINE_integer('sample_1_of_n_eval_on_train_examples', 5, 'Will sample '\n",
        "                     'one of every n train input examples for evaluation, '\n",
        "                     'where n is provided. This is only used if '\n",
        "                     '`eval_training_data` is True.')\n",
        "flags.DEFINE_string(\n",
        "    'hparams_overrides', None, 'Hyperparameter overrides, '\n",
        "    'represented as a string containing comma-separated '\n",
        "    'hparam_name=value pairs.')\n",
        "flags.DEFINE_string(\n",
        "    'checkpoint_dir', None, 'Path to directory holding a checkpoint.  If '\n",
        "    '`checkpoint_dir` is provided, this binary operates in eval-only mode, '\n",
        "    'writing resulting metrics to `model_dir`.')\n",
        "flags.DEFINE_boolean(\n",
        "    'run_once', True, 'If running in eval-only mode, whether to run just '\n",
        "    'one round of eval vs running continuously (default).'\n",
        ")\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "def main(unused_argv):\n",
        "  flags.mark_flag_as_required('model_dir')\n",
        "  flags.mark_flag_as_required('pipeline_config_path')\n",
        "  config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir, log_step_count_steps= 300, save_checkpoints_steps = 500)\n",
        "\n",
        "  train_and_eval_dict = model_lib.create_estimator_and_inputs(\n",
        "      run_config=config,\n",
        "      hparams=model_hparams.create_hparams(FLAGS.hparams_overrides),\n",
        "      pipeline_config_path=FLAGS.pipeline_config_path,\n",
        "      train_steps=FLAGS.num_train_steps,\n",
        "      sample_1_of_n_eval_examples=FLAGS.sample_1_of_n_eval_examples,\n",
        "      sample_1_of_n_eval_on_train_examples=(\n",
        "          FLAGS.sample_1_of_n_eval_on_train_examples))\n",
        "  estimator = train_and_eval_dict['estimator']\n",
        "  train_input_fn = train_and_eval_dict['train_input_fn']\n",
        "  eval_input_fns = train_and_eval_dict['eval_input_fns']\n",
        "  eval_on_train_input_fn = train_and_eval_dict['eval_on_train_input_fn']\n",
        "  predict_input_fn = train_and_eval_dict['predict_input_fn']\n",
        "  train_steps = train_and_eval_dict['train_steps']\n",
        "\n",
        "  if FLAGS.checkpoint_dir:\n",
        "    if FLAGS.eval_training_data:\n",
        "      name = 'training_data'\n",
        "      input_fn = eval_on_train_input_fn\n",
        "    else:\n",
        "      name = 'validation_data'\n",
        "      # The first eval input will be evaluated.\n",
        "      input_fn = eval_input_fns[0]\n",
        "    if FLAGS.run_once:\n",
        "      estimator.evaluate(input_fn,\n",
        "                         num_eval_steps=None,\n",
        "                         checkpoint_path=tf.train.latest_checkpoint(\n",
        "                             FLAGS.checkpoint_dir))\n",
        "    else:\n",
        "      model_lib.continuous_eval(estimator, FLAGS.checkpoint_dir, input_fn,\n",
        "                                train_steps, name)\n",
        "  else:\n",
        "    train_spec, eval_specs = model_lib.create_train_and_eval_specs(\n",
        "        train_input_fn,\n",
        "        eval_input_fns,\n",
        "        eval_on_train_input_fn,\n",
        "        predict_input_fn,\n",
        "        train_steps,\n",
        "        eval_on_train_data=False)\n",
        "\n",
        "    # Currently only a single Eval Spec is allowed.\n",
        "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtobkZ1-ci3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "def getFileFromGDrive():\n",
        "  # Authenticate and create the PyDrive client.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  # Load training files from drive, eed to provide <File_ID> thats has train and test files\n",
        "  file_list = drive.ListFile({'q': \"'File ID' in parents and trashed=false\"}).GetList()   \n",
        "  \n",
        "  file_name11 = file_list[0]['title']\n",
        "  print('training/test data: %s' % file_name11)\n",
        "  \n",
        "  for filename in file_list:\n",
        "    if not os.path.exists(filename['title']):\n",
        "      test_downloaded = drive.CreateFile({'id': filename['id']})\n",
        "      test_downloaded.GetContentFile(filename['title']) \n",
        "      print('Successfully loaded the files.')\n",
        "  return drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNX64QZ7eTz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getFileFromGDrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yEYeud9azC0b",
        "colab": {}
      },
      "source": [
        "%cd ~/datalab\n",
        "\n",
        "lst = os.listdir('/root/datalab/trained')\n",
        "lf = filter(lambda k: 'model.ckpt-' in k, lst)\n",
        "last_model = sorted(lf)[-1].replace('.meta', '')\n",
        "\n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config \\\n",
        "    --output_directory=fine_tuned_model \\\n",
        "    --trained_checkpoint_prefix=trained/$last_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkyLOWi-f1PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "files.download('/root/datalab/fine_tuned_model/frozen_inference_graph.pb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL6Zkfq_6jS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files\n",
        "\n",
        "def getFileFromGDrive():\n",
        "  # Authenticate and create the PyDrive client.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  return drive\n",
        "\n",
        "drive = getFileFromGDrive()\n",
        "uploaded = drive.CreateFile({'title':'frozen_inference_graph.pb'})\n",
        "uploaded.SetContentFile('/root/datalab/fine_tuned_model/frozen_inference_graph.pb')\n",
        "uploaded.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH4LJ6LMdHUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU1tLO2MzPZB",
        "colab_type": "code",
        "outputId": "71bb14ac-ed3c-44b6-e448-a6c03c542e9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "%cd /root\n",
        "from distutils.version import StrictVersion\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from utils import label_map_util\n",
        "\n",
        "from utils import visualization_utils as vis_util\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "from object_detection.utils import ops as utils_ops\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.GraphDef()\n",
        "  with tf.gfile.GFile('/root/datalab/fine_tuned_model/frozen_inference_graph.pb', 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "    \n",
        "category_index = label_map_util.create_category_index_from_labelmap('/root/person_label_map.pbtxt', use_display_name=True)\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "  \n",
        "def run_inference_for_single_image(image, graph):\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      # Get handles to input and output tensors\n",
        "      ops = tf.get_default_graph().get_operations()\n",
        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "      tensor_dict = {}\n",
        "      for key in [\n",
        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
        "          'detection_classes', 'detection_masks'\n",
        "      ]:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "              tensor_name)\n",
        "      if 'detection_masks' in tensor_dict:\n",
        "        # The following processing is only for single image\n",
        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "        detection_masks_reframed = tf.cast(\n",
        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "        # Follow the convention by adding back the batch dimension\n",
        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "            detection_masks_reframed, 0)\n",
        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "      # Run inference\n",
        "      output_dict = sess.run(tensor_dict,\n",
        "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "      output_dict['detection_classes'] = output_dict[\n",
        "          'detection_classes'][0].astype(np.uint8)\n",
        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "      if 'detection_masks' in output_dict:\n",
        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "  return output_dict\n",
        "\n",
        "\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (6, 5)\n",
        "\n",
        "for image_path in os.listdir(\"/root/data2/\"):\n",
        "  image = Image.open('/root/data2/'+image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "  plt.figure(figsize=IMAGE_SIZE)\n",
        "  plt.imshow(image_np)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "  max_open_warning, RuntimeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg2JCQJHeoOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /root/ssd_inception_v2_coco.config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDA671NKdjd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.listdir('/root/datalab/trained')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}